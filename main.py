from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_core.callbacks.base import BaseCallbackHandler
from dotenv import load_dotenv
import os
import json
import asyncio
from typing import Any, Dict, List

# ğŸ” í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ
load_dotenv()

# ğŸ”§ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ”§ CORS ì„¤ì • (ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë“  ë„ë©”ì¸ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬
class StreamingCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.tokens = []
        self.finished = False

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self.tokens.append(token)

    def on_llm_end(self, response, **kwargs: Any) -> None:
        self.finished = True

# ğŸ”§ ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°ìš© ëª¨ë¸ ì„¤ì •
streaming_llm = ChatOpenAI(
    model="gpt-4", 
    temperature=0.7,
    streaming=True
)

# ğŸ¯ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
full_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì˜ì–´ë¥¼ ì˜í•˜ê³  ì‹¶ì€ í•œêµ­ ë¶€ëª¨ë“¤ì„ ìœ„í•œ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì˜ì–´ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ë¶€ëª¨ëŠ” ì•„ì´ì™€ ì˜ì–´ë¡œ ëŒ€í™”í•˜ê¸° ìœ„í•´ ì—°ìŠµ ì¤‘ì´ë©°, ìˆ™ì œì²˜ëŸ¼ ì˜ì–´ ë¬¸ì¥ì„ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.
GPTì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ë¶€ëª¨ê°€ ì…ë ¥í•œ ë¬¸ì¥ì„ í™•ì¸í•˜ê³ , ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ê³ ì³ì£¼ì„¸ìš”. 
2. ìˆ˜ì •í•œ ì´ìœ ë¥¼ í•œêµ­ ë¶€ëª¨ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ê°„ë‹¨í•œ ë§ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
3. ì•„ì´ì™€ì˜ ì¼ìƒ ëŒ€í™”ì—ì„œ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ê´€ë ¨ ì˜ì–´ í‘œí˜„ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. 
4. ë§Œì•½ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•¨ê»˜ ì…ë ¥í–ˆë‹¤ë©´, ê·¸ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
   (ì§ˆë¬¸ì€ ì„ íƒ í•­ëª©ì´ë¯€ë¡œ, ì…ë ¥ì´ ì—†ìœ¼ë©´ ìƒëµí•˜ì„¸ìš”.)
5. ì˜ëª»ëœ ë¶€ë¶„ì„ ê³ ì³ì£¼ëŠ”ê²ƒê³¼ ì•„ì´ì™€ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ì˜ì–´í‘œí˜„ 3ê°€ì§€ì— ëŒ€í•´ì„œ í•­ìƒ ê·¸ ëœ»ë„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
# 6. ê°€ì¥ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ ì˜ í•˜ê³ ìˆë‹¤ëŠ”, í˜¹ì€ ë‹¤ë¥¸ í‘œí˜„ì˜ ì‘ì›ì˜ ë§ë¡œ ë§ˆì§€ë§‰ì„ ì¥ì‹í•´ì£¼ì„¸ìš”. ì¢€ ë¶€ë“œëŸ½ê²Œ ì‘ì›í•´ì£¼ì„¸ìš”.
ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì…ë‹ˆë‹¤:

ë¬¸ì¥: "{english_essay}"

ì§ˆë¬¸ (ì„ íƒ): "{question}"

ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì£¼ì„¸ìš”:

âœ… ê³ ì³ì¤€ ë¬¸ì¥

ğŸ§  ìˆ˜ì •í•œ ì´ìœ 

ğŸ’¬ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” í‘œí˜„ 3ê°€ì§€

â“ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (ì§ˆë¬¸ì´ ìˆì„ ê²½ìš°ì—ë§Œ)
""")

feedback_chain = LLMChain(llm=llm, prompt=full_prompt)

# âœ… ì…ë ¥ ë° ì¶œë ¥ ë°ì´í„° ëª¨ë¸ ì •ì˜
class FeedbackRequest(BaseModel):
    english_essay: str
    question: str = ""

class FeedbackResponse(BaseModel):
    result: str

# âœ… í”¼ë“œë°± ìƒì„± API (ê¸°ì¡´)
@app.post("/generate-feedback", response_model=FeedbackResponse)
async def generate_feedback(data: FeedbackRequest):
    response = feedback_chain.invoke({
        "english_essay": data.english_essay,
        "question": data.question
    })
    return FeedbackResponse(result=response["text"])

# ğŸš€ ìŠ¤íŠ¸ë¦¬ë° í”¼ë“œë°± ìƒì„± API (ìƒˆë¡œ ì¶”ê°€)
@app.post("/stream-feedback")
async def stream_feedback(data: FeedbackRequest):
    async def generate():
        try:
            # ì‹œì‘ ë©”ì‹œì§€
            yield f"data: {json.dumps({'status': 'starting', 'message': 'AI í”¼ë“œë°± ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
            callback_handler = StreamingCallbackHandler()
            
            # ìŠ¤íŠ¸ë¦¬ë°ìš© LLM ì²´ì¸ ìƒì„±
            streaming_chain = LLMChain(
                llm=ChatOpenAI(
                    model="gpt-4", 
                    temperature=0.7,
                    streaming=True,
                    callbacks=[callback_handler]
                ), 
                prompt=full_prompt
            )
            
            # ì§„í–‰ ìƒíƒœ ë©”ì‹œì§€
            yield f"data: {json.dumps({'status': 'processing', 'message': 'GPT-4ê°€ í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # í”¼ë“œë°± ìƒì„± ì‹œì‘
            response = await asyncio.to_thread(
                streaming_chain.invoke,
                {
                    "english_essay": data.english_essay,
                    "question": data.question
                }
            )
            
            # ì „ì²´ ì‘ë‹µì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì „ì†¡
            full_text = response["text"]
            chunk_size = 100  # í•œ ë²ˆì— ë³´ë‚¼ ë¬¸ì ìˆ˜
            
            for i in range(0, len(full_text), chunk_size):
                chunk = full_text[i:i + chunk_size]
                chunk_data = json.dumps({"content": chunk}, ensure_ascii=False)
                yield f"data: {chunk_data}\n\n"
                await asyncio.sleep(0.05)  # ì•½ê°„ì˜ ì§€ì—°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            
            # ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: {json.dumps({'done': True, 'message': 'í”¼ë“œë°± ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬
            error_data = json.dumps({"error": f"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
            yield f"data: {error_data}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Content-Type,Authorization",
        }
    )


# í…ŒìŠ¤íŠ¸
async def stream_response():
    for i in range(5):
        yield f"data: {json.dumps({'content': f'Chunk {i}'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(1)

@app.post("/api/test-stream")
async def test_stream():
    return StreamingResponse(stream_response(), media_type="text/event-stream")