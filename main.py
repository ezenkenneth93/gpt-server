from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_core.callbacks.base import BaseCallbackHandler
from dotenv import load_dotenv
import os
import json
import asyncio

from asyncio import create_task
from asyncio import Queue   
from typing import Any, Dict, List

# ì›ë˜ ìƒíƒœë¡œ ë³µì›

# ğŸ” í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ
load_dotenv()

# ğŸ”§ OPTION 4: ì—°ê²° í’€ë§ì€ ì¼ë‹¨ ì œê±° (í˜¸í™˜ì„± ë¬¸ì œ)

# ğŸ”§ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ”§ CORS ì„¤ì • (ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë“  ë„ë©”ì¸ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ (ë°±ì—…ìš© - ê¸°ì¡´ ê°€ì§œ ìŠ¤íŠ¸ë¦¬ë°)
# class StreamingCallbackHandler(BaseCallbackHandler):
#     def __init__(self):
#         self.tokens = []
#         self.finished = False

#     def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
#         self.tokens.append(token)

#     def on_llm_end(self, response, **kwargs: Any) -> None:
#         self.finished = True

# ğŸš€ ì§„ì§œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹„ë™ê¸° í ì½œë°± í•¸ë“¤ëŸ¬
class AsyncQueueCallbackHandler(BaseCallbackHandler):
    def __init__(self, queue: Queue):
        self.queue = queue
        self.loop = asyncio.get_event_loop()  # ë©”ì¸ ë£¨í”„ ì €ì¥

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        # ë©”ì¸ ë£¨í”„ì— ì½”ë£¨í‹´ ì•ˆì „í•˜ê²Œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        asyncio.run_coroutine_threadsafe(self.queue.put(token), self.loop)

    def on_llm_end(self, response, **kwargs: Any) -> None:
        asyncio.run_coroutine_threadsafe(self.queue.put(None), self.loop)


# ğŸ”§ ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(
    model="gpt-4o", 
    temperature=0.7
)

# ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°ìš© ëª¨ë¸ ì„¤ì • (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ - ì¬ì‚¬ìš©)
streaming_llm = ChatOpenAI(
    model="gpt-4o", 
    temperature=0.7,
    streaming=True
)

# ğŸ¯ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
full_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì˜ì–´ë¥¼ ì˜í•˜ê³  ì‹¶ì€ í•œêµ­ ë¶€ëª¨ë“¤ì„ ìœ„í•œ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì˜ì–´ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ë¶€ëª¨ëŠ” ì•„ì´ì™€ ì˜ì–´ë¡œ ëŒ€í™”í•˜ê¸° ìœ„í•´ ì—°ìŠµ ì¤‘ì´ë©°, ìˆ™ì œì²˜ëŸ¼ ì˜ì–´ ë¬¸ì¥ì„ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.
GPTì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ë¶€ëª¨ê°€ ì…ë ¥í•œ ë¬¸ì¥ì„ í™•ì¸í•˜ê³ , ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ê³ ì³ì£¼ì„¸ìš”. 
2. ìˆ˜ì •í•œ ì´ìœ ë¥¼ í•œêµ­ ë¶€ëª¨ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ê°„ë‹¨í•œ ë§ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
3. ì•„ì´ì™€ì˜ ì¼ìƒ ëŒ€í™”ì—ì„œ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ê´€ë ¨ ì˜ì–´ í‘œí˜„ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. 
4. ë§Œì•½ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•¨ê»˜ ì…ë ¥í–ˆë‹¤ë©´, ê·¸ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
   (ì§ˆë¬¸ì€ ì„ íƒ í•­ëª©ì´ë¯€ë¡œ, ì…ë ¥ì´ ì—†ìœ¼ë©´ ìƒëµí•˜ì„¸ìš”.)
5. ì˜ëª»ëœ ë¶€ë¶„ì„ ê³ ì³ì£¼ëŠ”ê²ƒê³¼ ì•„ì´ì™€ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ì˜ì–´í‘œí˜„ 3ê°€ì§€ì— ëŒ€í•´ì„œ í•­ìƒ ê·¸ ëœ»ë„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
# 6. ê°€ì¥ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ ì˜ í•˜ê³ ìˆë‹¤ëŠ”, í˜¹ì€ ë‹¤ë¥¸ í‘œí˜„ì˜ ì‘ì›ì˜ ë§ë¡œ ë§ˆì§€ë§‰ì„ ì¥ì‹í•´ì£¼ì„¸ìš”. ì¢€ ë¶€ë“œëŸ½ê²Œ ì‘ì›í•´ì£¼ì„¸ìš”.
7. ì‘ë‹µì„ ì¶œë ¥í•˜ëŠ” ì—ë””í„°ëŠ” ë§ˆí¬ì—…, ë§ˆí¬ë‹¤ìš´ì´ ì•„ë‹ˆë¯€ë¡œ ê°•ì¡°í•  ë–„ ** ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì…ë‹ˆë‹¤:

ë¬¸ì¥: "{english_essay}"

ì§ˆë¬¸ (ì„ íƒ): "{question}"

ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì£¼ì„¸ìš”:

âœ… ê³ ì³ì¤€ ë¬¸ì¥

ğŸ§  ìˆ˜ì •í•œ ì´ìœ 

ğŸ’¬ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” í‘œí˜„ 3ê°€ì§€

â“ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (ì§ˆë¬¸ì´ ìˆì„ ê²½ìš°ì—ë§Œ)
""")

feedback_chain = LLMChain(llm=llm, prompt=full_prompt)

# ğŸš€ OPTION 9: Pre-warming (ì˜ˆì—´) - ì•± ì‹œì‘ ì‹œ ì—°ê²° ë¯¸ë¦¬ ì¤€ë¹„
@app.on_event("startup")
async def startup_event():
    try:
        print("ğŸ”¥ Pre-warming: OpenAI ì—°ê²° ì¤€ë¹„ ì¤‘...")
        # ë”ë¯¸ ìš”ì²­ìœ¼ë¡œ ì—°ê²° ì˜ˆì—´
        dummy_response = llm.invoke("Hello")
        print("âœ… Pre-warming ì™„ë£Œ: OpenAI ì—°ê²° ì¤€ë¹„ë¨!")
    except Exception as e:
        print(f"âš ï¸ Pre-warming ì‹¤íŒ¨ (ì •ìƒ ë™ì‘ì—ëŠ” ì˜í–¥ ì—†ìŒ): {e}")

@app.on_event("shutdown")
async def shutdown_event():
    print("ğŸ”„ ì•± ì¢…ë£Œ ì¤‘...")
    print("âœ… ì •ë¦¬ ì™„ë£Œ!")

# âœ… ì…ë ¥ ë° ì¶œë ¥ ë°ì´í„° ëª¨ë¸ ì •ì˜
class FeedbackRequest(BaseModel):
    english_essay: str
    question: str = ""

class FeedbackResponse(BaseModel):
    result: str

# âœ… í”¼ë“œë°± ìƒì„± API (ê¸°ì¡´)
@app.post("/generate-feedback", response_model=FeedbackResponse)
async def generate_feedback(data: FeedbackRequest):
    response = feedback_chain.invoke({
        "english_essay": data.english_essay,
        "question": data.question
    })
    return FeedbackResponse(result=response["text"])

# ğŸš€ ìŠ¤íŠ¸ë¦¬ë° í”¼ë“œë°± ìƒì„± API (ìƒˆë¡œ ì¶”ê°€)
@app.post("/stream-feedback")
async def stream_feedback(data: FeedbackRequest):
    async def generate():
        try:
            # ğŸš€ OPTION 10: ì¦‰ì‹œ ì‘ë‹µ ì‹œì‘ - ì—°ê²° í™•ì¸ ë©”ì‹œì§€ë¶€í„° ì „ì†¡
            yield f"data: {json.dumps({'status': 'connected', 'message': 'ì—°ê²° ì™„ë£Œ! í”¼ë“œë°± ìƒì„± ì¤€ë¹„ ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
            # callback_handler = StreamingCallbackHandler() # ê¸°ì¡´ ê°€ì§œ ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
            queue_instance = Queue()   # ì§„ì§œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í (thread-safe)
            callback_handler = AsyncQueueCallbackHandler(queue_instance)
            
            # ğŸš€ ì½œë°±ì´ í¬í•¨ëœ ìŠ¤íŠ¸ë¦¬ë° LLM ìƒì„± (ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œ ìƒì„±, ì½œë°± í¬í•¨)
            streaming_llm_with_callback = ChatOpenAI(
                model="gpt-4o", 
                temperature=0.7,
                streaming=True,
                callbacks=[callback_handler]  # ChatOpenAI ë ˆë²¨ì—ì„œ ì½œë°± ì„¤ì •
            )
            
            # ì„¤ì • ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: {json.dumps({'status': 'ready', 'message': 'ì„¤ì • ì™„ë£Œ! AI ë¶„ì„ ì‹œì‘...'}, ensure_ascii=False)}\n\n"
            
            # ğŸš€ ìŠ¤íŠ¸ë¦¬ë°ìš© LLM ì²´ì¸ ìƒì„±
            streaming_chain = LLMChain(
                llm=streaming_llm_with_callback,  # ì½œë°±ì´ í¬í•¨ëœ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
                prompt=full_prompt
            )
            
            # ì§„í–‰ ìƒíƒœ ë©”ì‹œì§€
            yield f"data: {json.dumps({'status': 'processing', 'message': 'GPT-4 Turboê°€ í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            # ğŸš€ ì§„ì§œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°: LLM ì²´ì¸ì„ ë³„ë„ íƒœìŠ¤í¬ì—ì„œ ì‹¤í–‰
            async def run_chain():
                await asyncio.to_thread(
                    streaming_chain.invoke,
                    {
                        "english_essay": data.english_essay,
                        "question": data.question
                    }
                )
            
            # ì²´ì¸ ì‹¤í–‰ ì‹œì‘
            chain_task = asyncio.create_task(run_chain())
            
            # ğŸš€ í† í° ìŠ¤íŠ¸ë¦¬ë°: íì—ì„œ í† í°ì„ ë°›ëŠ” ì¦‰ì‹œ ë°”ë¡œ ì „ì†¡
            while True:
                # thread-safe queueì—ì„œ non-blockingìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                token = await queue_instance.get()
                if token is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                yield f"data: {json.dumps({'content': token}, ensure_ascii=False)}\n\n"
            
            # ì™„ë£Œ ë©”ì‹œì§€
            yield f"data: {json.dumps({'done': True, 'message': 'í”¼ë“œë°± ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬
            error_data = json.dumps({"error": f"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
            yield f"data: {error_data}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Content-Type,Authorization",
        }
    )


# í…ŒìŠ¤íŠ¸
async def stream_response():
    for i in range(5):
        yield f"data: {json.dumps({'content': f'Chunk {i}'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(1)

@app.post("/api/test-stream")
async def test_stream():
    return StreamingResponse(stream_response(), media_type="text/event-stream")