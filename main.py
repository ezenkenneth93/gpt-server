from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_core.callbacks.base import BaseCallbackHandler
from dotenv import load_dotenv
from fastapi import WebSocket, WebSocketDisconnect
import os
import json
import asyncio

from asyncio import Queue
from typing import Any

# ğŸ” í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ
load_dotenv()

# ğŸ”§ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ”§ CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬ (ë¹„ë™ê¸° í ë°©ì‹)
class AsyncQueueCallbackHandler(BaseCallbackHandler):
    def __init__(self, queue: Queue):
        self.queue = queue
        self.loop = asyncio.get_event_loop()

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        asyncio.run_coroutine_threadsafe(self.queue.put(token), self.loop)

    def on_llm_end(self, response, **kwargs: Any) -> None:
        asyncio.run_coroutine_threadsafe(self.queue.put(None), self.loop)

# ğŸ”§ ì¼ë°˜ LLM (ê¸°ë³¸)
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7
)

# ğŸ”§ ì „ì—­ ìŠ¤íŠ¸ë¦¬ë° LLM ì¸ìŠ¤í„´ìŠ¤ (ì½œë°± ì—†ì´ ì¬ì‚¬ìš©)
base_streaming_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    streaming=True
)

# âœ… ì½œë°±ë§Œ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì—¬ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
def get_llm_with_callback(callback_handler: BaseCallbackHandler) -> ChatOpenAI:
    return ChatOpenAI(
        model=base_streaming_llm.model_name,
        temperature=base_streaming_llm.temperature,
        streaming=True,
        callbacks=[callback_handler]
    )

# ğŸ”§ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
full_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì˜ì–´ë¥¼ ì˜í•˜ê³  ì‹¶ì€ í•œêµ­ ë¶€ëª¨ë“¤ì„ ìœ„í•œ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì˜ì–´ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ë¶€ëª¨ëŠ” ì•„ì´ì™€ ì˜ì–´ë¡œ ëŒ€í™”í•˜ê¸° ìœ„í•´ ì—°ìŠµ ì¤‘ì´ë©°, ìˆ™ì œì²˜ëŸ¼ ì˜ì–´ ë¬¸ì¥ì„ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.
GPTì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ë¶€ëª¨ê°€ ì…ë ¥í•œ ë¬¸ì¥ì„ í™•ì¸í•˜ê³ , ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ê³ ì³ì£¼ì„¸ìš”. 
2. ìˆ˜ì •í•œ ì´ìœ ë¥¼ í•œêµ­ ë¶€ëª¨ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ê°„ë‹¨í•œ ë§ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
3. ì•„ì´ì™€ì˜ ì¼ìƒ ëŒ€í™”ì—ì„œ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ê´€ë ¨ ì˜ì–´ í‘œí˜„ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. 
4. ë§Œì•½ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•¨ê»˜ ì…ë ¥í–ˆë‹¤ë©´, ê·¸ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
   (ì§ˆë¬¸ì€ ì„ íƒ í•­ëª©ì´ë¯€ë¡œ, ì…ë ¥ì´ ì—†ìœ¼ë©´ ìƒëµí•˜ì„¸ìš”.)
5. ì˜ëª»ëœ ë¶€ë¶„ì„ ê³ ì³ì£¼ëŠ”ê²ƒê³¼ ì•„ì´ì™€ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ì˜ì–´í‘œí˜„ 3ê°€ì§€ì— ëŒ€í•´ì„œ í•­ìƒ ê·¸ ëœ»ë„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
6. ê°€ì¥ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ ì˜ í•˜ê³ ìˆë‹¤ëŠ”, í˜¹ì€ ë‹¤ë¥¸ í‘œí˜„ì˜ ì‘ì›ì˜ ë§ë¡œ ë§ˆì§€ë§‰ì„ ì¥ì‹í•´ì£¼ì„¸ìš”. ì¢€ ë¶€ë“œëŸ½ê²Œ ì‘ì›í•´ì£¼ì„¸ìš”.
7. ì‘ë‹µì„ ì¶œë ¥í•˜ëŠ” ì—ë””í„°ëŠ” ë§ˆí¬ì—…, ë§ˆí¬ë‹¤ìš´ì´ ì•„ë‹ˆë¯€ë¡œ ê°•ì¡°í•  ë–„ ** ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì…ë‹ˆë‹¤:

ë¬¸ì¥: "{english_essay}"

ì§ˆë¬¸ (ì„ íƒ): "{question}"

ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì£¼ì„¸ìš”:

âœ… ê³ ì³ì¤€ ë¬¸ì¥

ğŸ§  ìˆ˜ì •í•œ ì´ìœ 

ğŸ’¬ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” í‘œí˜„ 3ê°€ì§€

â“ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (ì§ˆë¬¸ì´ ìˆì„ ê²½ìš°ì—ë§Œ)
""")

# âœ… ì¼ë°˜ í”¼ë“œë°±ìš© ì²´ì¸
feedback_chain = LLMChain(llm=llm, prompt=full_prompt)

# âœ… ë°ì´í„° ëª¨ë¸ ì •ì˜
class FeedbackRequest(BaseModel):
    english_essay: str
    question: str = ""

class FeedbackResponse(BaseModel):
    result: str

# âœ… ì¼ë°˜ í”¼ë“œë°± API
@app.post("/generate-feedback", response_model=FeedbackResponse)
async def generate_feedback(data: FeedbackRequest):
    response = feedback_chain.invoke({
        "english_essay": data.english_essay,
        "question": data.question
    })
    return FeedbackResponse(result=response["text"])

# âœ… ìŠ¤íŠ¸ë¦¬ë° í”¼ë“œë°± API
# âœ… ìŠ¤íŠ¸ë¦¬ë° í”¼ë“œë°± API
@app.post("/stream-feedback")
async def stream_feedback(data: FeedbackRequest):
    print("ğŸ”¥ ìš”ì²­ ìˆ˜ì‹ ë¨!")
    print("ğŸ“ Essay:", data.english_essay[:50])  # ì¼ë¶€ë§Œ ì¶œë ¥
    print("â“ Question:", data.question[:50])    # ì¼ë¶€ë§Œ ì¶œë ¥

    async def generate():
        try:
            print("âš™ï¸ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸° ì‹œì‘ë¨")

            yield f"data: {json.dumps({'status': 'connected', 'message': 'ì—°ê²° ì™„ë£Œ! í”¼ë“œë°± ìƒì„± ì¤€ë¹„ ì¤‘...'}, ensure_ascii=False)}\n\n"

            queue_instance = Queue()
            callback_handler = AsyncQueueCallbackHandler(queue_instance)

            streaming_llm_with_callback = get_llm_with_callback(callback_handler)
            print("âœ… LLM ì½œë°± í•¸ë“¤ëŸ¬ ì„¸íŒ… ì™„ë£Œ")

            yield f"data: {json.dumps({'status': 'ready', 'message': 'ì„¤ì • ì™„ë£Œ! AI ë¶„ì„ ì‹œì‘...'}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'status': 'processing', 'message': 'GPTê°€ í”¼ë“œë°±ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"

            prompt_text = full_prompt.format(
                english_essay=data.english_essay,
                question=data.question
            )
            print("ğŸ“¤ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")

            # ì²´ì¸ ì‹¤í–‰ (ì“°ë ˆë“œì—ì„œ)
            async def run_chain():
                print("ğŸš€ GPT í˜¸ì¶œ ì‹œì‘")
                await asyncio.to_thread(
                    streaming_llm_with_callback.invoke,
                    prompt_text
                )
                print("âœ… GPT í˜¸ì¶œ ì™„ë£Œ")

            asyncio.create_task(run_chain())

            # íì—ì„œ í† í° ìˆ˜ì‹ 
            while True:
                token = await queue_instance.get()
                # print("ğŸ“¥ ìˆ˜ì‹ ëœ í† í°:", token)
                if token is None:
                    print("ğŸ”š í† í° ìˆ˜ì‹  ì¢…ë£Œ")
                    break
                yield f"data: {json.dumps({'content': token}, ensure_ascii=False)}\n\n"

            yield f"data: {json.dumps({'done': True, 'message': 'í”¼ë“œë°± ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
            print("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")

        except Exception as e:
            print("âŒ ì˜ˆì™¸ ë°œìƒ:", str(e))
            yield f"data: {json.dumps({'error': f'í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}'}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Content-Type,Authorization",
        }
    )


# âœ… ì˜ˆì—´
@app.on_event("startup")
async def startup_event():
    try:
        print("ğŸ”¥ Pre-warming: OpenAI ì—°ê²° ì¤€ë¹„ ì¤‘...")
        llm.invoke("Hello")
        print("âœ… Pre-warming ì™„ë£Œ: ëª¨ë¸ ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        print(f"âš ï¸ Pre-warming ì‹¤íŒ¨: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    print("ğŸ”„ ì•± ì¢…ë£Œ ì¤‘...")
    print("âœ… ì •ë¦¬ ì™„ë£Œ!")

# websocket ì—°ê²°
@app.websocket("/ws-feedback")
async def websocket_feedback(websocket: WebSocket):
    await websocket.accept()

    try:
        # í´ë¼ì´ì–¸íŠ¸ê°€ ë³´ë‚¸ JSON íŒŒì‹±
        data = await websocket.receive_text()
        payload = json.loads(data)
        essay = payload.get("english_essay", "")
        question = payload.get("question", "")

        # í/ì½œë°± êµ¬ì„±
        queue_instance = Queue()
        callback_handler = AsyncQueueCallbackHandler(queue_instance)
        streaming_llm_with_callback = get_llm_with_callback(callback_handler)

        # í”„ë¡¬í”„íŠ¸ ë Œë”ë§
        prompt_text = full_prompt.format(
            english_essay=essay,
            question=question
        )

        # LLM ì‹¤í–‰ (ë¹„ë™ê¸°)
        async def run_chain():
            await asyncio.to_thread(
                streaming_llm_with_callback.invoke,
                prompt_text
            )

        asyncio.create_task(run_chain())

        # íì—ì„œ í† í° ìˆ˜ì‹  â†’ WebSocket ì „ì†¡
        while True:
            token = await queue_instance.get()
            if token is None:
                break
            await websocket.send_text(token)

        await websocket.send_text("__END__")

    except WebSocketDisconnect:
        print("âŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œë¨")
    except Exception as e:
        await websocket.send_text(f"__ERROR__: {str(e)}")
        await websocket.close()
