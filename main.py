from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from dotenv import load_dotenv
import os

# ğŸ” í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ
load_dotenv()

# ğŸ”§ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ”§ CORS ì„¤ì • (ê°œë°œ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë“  ë„ë©”ì¸ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ”§ ëª¨ë¸ ì„¤ì •
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# ğŸ¯ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
full_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì˜ì–´ë¥¼ ì˜í•˜ê³  ì‹¶ì€ í•œêµ­ ë¶€ëª¨ë“¤ì„ ìœ„í•œ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì˜ì–´ ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ë¶€ëª¨ëŠ” ì•„ì´ì™€ ì˜ì–´ë¡œ ëŒ€í™”í•˜ê¸° ìœ„í•´ ì—°ìŠµ ì¤‘ì´ë©°, ìˆ™ì œì²˜ëŸ¼ ì˜ì–´ ë¬¸ì¥ì„ ì…ë ¥í–ˆìŠµë‹ˆë‹¤.
GPTì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ë¶€ëª¨ê°€ ì…ë ¥í•œ ë¬¸ì¥ì„ í™•ì¸í•˜ê³ , ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ê³ ì³ì£¼ì„¸ìš”. 
2. ìˆ˜ì •í•œ ì´ìœ ë¥¼ í•œêµ­ ë¶€ëª¨ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê³  ê°„ë‹¨í•œ ë§ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
3. ì•„ì´ì™€ì˜ ì¼ìƒ ëŒ€í™”ì—ì„œ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ê´€ë ¨ ì˜ì–´ í‘œí˜„ 3ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. 
4. ë§Œì•½ ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•¨ê»˜ ì…ë ¥í–ˆë‹¤ë©´, ê·¸ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
   (ì§ˆë¬¸ì€ ì„ íƒ í•­ëª©ì´ë¯€ë¡œ, ì…ë ¥ì´ ì—†ìœ¼ë©´ ìƒëµí•˜ì„¸ìš”.)
5. ì˜ëª»ëœ ë¶€ë¶„ì„ ê³ ì³ì£¼ëŠ”ê²ƒê³¼ ì•„ì´ì™€ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” ì˜ì–´í‘œí˜„ 3ê°€ì§€ì— ëŒ€í•´ì„œ í•­ìƒ ê·¸ ëœ»ë„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
6. ê°€ì¥ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ ì˜ í•˜ê³ ìˆë‹¤ëŠ”, í˜¹ì€ ë‹¤ë¥¸ í‘œí˜„ì˜ ì‘ì›ì˜ ë§ë¡œ ë§ˆì§€ë§‰ì„ ì¥ì‹í•´ì£¼ì„¸ìš”. ì¢€ ë¶€ë“œëŸ½ê²Œ ì‘ì›í•´ì£¼ì„¸ìš”.
ì•„ë˜ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì…ë‹ˆë‹¤:

ë¬¸ì¥: "{english_essay}"

ì§ˆë¬¸ (ì„ íƒ): "{question}"

ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì£¼ì„¸ìš”:

âœ… ê³ ì³ì¤€ ë¬¸ì¥

ğŸ§  ìˆ˜ì •í•œ ì´ìœ 

ğŸ’¬ í•¨ê»˜ ì“¸ ìˆ˜ ìˆëŠ” í‘œí˜„ 3ê°€ì§€

â“ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (ì§ˆë¬¸ì´ ìˆì„ ê²½ìš°ì—ë§Œ)
""")

feedback_chain = LLMChain(llm=llm, prompt=full_prompt)

# âœ… ì…ë ¥ ë° ì¶œë ¥ ë°ì´í„° ëª¨ë¸ ì •ì˜
class FeedbackRequest(BaseModel):
    english_essay: str
    question: str = ""

class FeedbackResponse(BaseModel):
    result: str

# âœ… í”¼ë“œë°± ìƒì„± API
@app.post("/generate-feedback", response_model=FeedbackResponse)
async def generate_feedback(data: FeedbackRequest):
    response = feedback_chain.invoke({
        "english_essay": data.english_essay,
        "question": data.question
    })
    return FeedbackResponse(result=response["text"])